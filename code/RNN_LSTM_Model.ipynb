{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVLp1Ag1o8Im"
   },
   "source": [
    "# **Predicting U.S. Crime Rates**\n",
    "\n",
    "## **Recurrent Neural Network with Long Short-Term Memory**\n",
    "\n",
    "---\n",
    "\n",
    "**PLEASE NOTE: To run this notebook, the user may need to upload this notebook along with the following dataset to [Google Colab](https://colab.research.google.com/). From what we can tell, an incompatibility between numpy and Tensorflow prevents the script from running in Jupyter.**\n",
    "\n",
    "* Upload the [predictors_and_targets.csv](../data/model_inputs/predictors_and_targets.csv) to the Colab runtime. The `pd.read_csv()` call below is configured to read the csv from the root runtime upload folder. The this notebook writes the model output to .csvs which can be downloaded from the same runtime folder.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we'll seek to train an Long Short-Term Memory Recurrent Neural Network in a fashion inspired by [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series#setup).\n",
    "\n",
    "Start by importing a few important libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cHPcats3o9Lr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "# set a random seed:\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS0UN_pcpBOJ"
   },
   "source": [
    "Import and format the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9Vq_aDG7o_m9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('predictors_and_targets.csv')\n",
    "to_drop = ['violent_crime', 'homicide', 'rape',\n",
    "       'robbery', 'aggravated_assault', 'property_crime', 'burglary',\n",
    "       'larceny', 'motor_vehicle_theft', 'arson', 'homicide_1000', 'rape_1000', \n",
    "       'robbery_1000', 'aggravated_assault_1000','burglary_1000', 'larceny_1000',\n",
    "       'motor_vehicle_theft_1000', 'arson_1000', 'ag_Unknown']\n",
    "df.drop(columns=to_drop,inplace=True)\n",
    "\n",
    "#to datetime format inspired by user Zero: https://stackoverflow.com/a/46658244\n",
    "years = df.copy()\n",
    "years = list(years['year'].unique())\n",
    "years.append(years[-1]+1)\n",
    "years = pd.to_datetime(np.array(years),format=\"%Y\")\n",
    "\n",
    "#set the years to a datetime format:\n",
    "df['year'] = pd.to_datetime(df['year'], format = '%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjDZUrOQpKhr"
   },
   "source": [
    "## Modeling Goals\n",
    "* Build a model that can produce predictions for a specified crime rate in each state for every year after the first observed year. Do this for two crime categories:\n",
    "    * `violent_crime_1000`, the number of violent crimes committed per thousand population in a state and\n",
    "    * `property_crime_1000`, the number of property crimes committed per thousand population in a state.\n",
    "* Compare the results of the model to a baseline for evaluation  \n",
    "  * Metrics used will be MAE, RMSE, and R2\n",
    "* Append the predictions to a time series of crime rates\n",
    "  * Flag the predictions as 'forecast' and the observed as 'historical'\n",
    "\n",
    "---\n",
    "\n",
    "Modeling steps that must happen for each state:\n",
    "\n",
    "1. Extract the state dataframe\n",
    "1. Set the year column to be the index\n",
    "1. Define the X and Y\n",
    "1. Set aside just the **index** and **crime rate** columns in another dataframe, with a new field which labels the observations as 'historical'. Add a new row for the forecast period, with values to be filled later.\n",
    "1. Scale the Data\n",
    "1. Make predictions using baseline model, add to scoring matrix\n",
    "1. Make predictions using LSTM model, add to scoring matrix\n",
    "1. Append predictions to historical+forecast dataframe\n",
    "\n",
    "We will run the above for each state plus D.C. We'll evaluate the model based on mean MAE, RMSE, and R2 of all the state's predictions VS the mean MAE, RMSE, and R2 of all the states predicted using the baseline model.\n",
    "\n",
    "First, we'll need to define a helper class, which will generate windows of consecutive time steps. Heavily inspired by [TensorFlow Time Series tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series#setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4Q6i-8cMpK4S"
   },
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df,\n",
    "                 label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    #WindowGenerator.split_window = split_window\n",
    "\n",
    "    def plot(self, model=None, plot_col='violent_crime_1000', max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                 label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                            marker='X', edgecolors='k', label='Predictions',\n",
    "                            c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Year')\n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=41,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "#define a modeling function to fit a couple neural nets:\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=patience,\n",
    "                                                      mode='min')\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                        validation_data=window.train,\n",
    "                        callbacks=[early_stopping], verbose=0)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWFNXIIKu2bC",
    "outputId": "a50970d8-39fb-479d-d6f5-38ee7410df94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target_forecast: (0, 5) (['year', 'state', 'measure_type', 'violent_crime_1000', 'property_crime_1000'])\n",
      "shape of scores: ((0, 8), Index(['state', 'crime_rate', 'baseline_MAE', 'baseline_RMSE', 'baseline_R2',\n",
      "       'LSTM_MAE', 'LSTM_RMSE', 'LSTM_R2'],\n",
      "      dtype='object')) (['state', 'crime_rate', 'baseline_MAE', 'baseline_RMSE', 'baseline_R2', 'LSTM_MAE', 'LSTM_RMSE', 'LSTM_R2'])\n",
      "shape of rate_preds: ((0, 4), Index(['year', 'state', 'crime_rate', 'prediction'], dtype='object')) (['year', 'state', 'crime_rate', 'prediction'])\n"
     ]
    }
   ],
   "source": [
    "#define our states:\n",
    "states = df.copy()\n",
    "states = states['state_abbr'].unique()\n",
    "\n",
    "#define dataframes to hold new things:\n",
    "target_forecast = pd.DataFrame(None,columns=['year','state','measure_type','violent_crime_1000','property_crime_1000'])\n",
    "print(f'shape of target_forecast: {target_forecast.shape} ({list(target_forecast.columns)})') # will add to this as we loop through modeling the states\n",
    "\n",
    "scores = pd.DataFrame(None,columns=['state','crime_rate','baseline_MAE','baseline_RMSE','baseline_R2','LSTM_MAE','LSTM_RMSE','LSTM_R2'])\n",
    "print(f'shape of scores: {scores.shape, scores.columns} ({list(scores.columns)})') # will add to this as we loop through modeling\n",
    "\n",
    "rate_preds = pd.DataFrame(None,columns=['year','state','crime_rate','prediction'])\n",
    "print(f'shape of rate_preds: {rate_preds.shape, rate_preds.columns} ({list(rate_preds.columns)})')\n",
    "\n",
    "def get_scores(state, crime, true, baseline, pred):\n",
    "    state_scores = {x:None for x in scores.columns}\n",
    "    state_scores['state'] = state\n",
    "    state_scores['crime_rate'] = crime\n",
    "    state_scores['baseline_MAE'] = metrics.mean_absolute_error(true, baseline)\n",
    "    state_scores['baseline_RMSE'] = metrics.mean_squared_error(true, baseline,squared=False)\n",
    "    state_scores['baseline_R2'] = metrics.r2_score(true, baseline)\n",
    "    state_scores['LSTM_MAE'] = metrics.mean_absolute_error(true, pred)\n",
    "    state_scores['LSTM_RMSE'] = metrics.mean_squared_error(true, pred,squared=False)\n",
    "    state_scores['LSTM_R2'] = metrics.r2_score(true, pred)\n",
    "    return state_scores # must be a dict with keys matching the scores columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIF60UBIu6V1",
    "outputId": "35c3a654-db26-476e-ab7a-7bb02212d0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9c171c8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9c171c8d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    state_crime = df.copy()\n",
    "    state_crime = state_crime[state_crime['state_abbr']==state]\n",
    "    state_crime.drop(columns=['state_abbr'],inplace=True)\n",
    "    state_crime.set_index('year',inplace=True)\n",
    "\n",
    "    #save the mean and standard deviation to unscale predictions later:\n",
    "    means = {'violent_crime_1000': state_crime['violent_crime_1000'].mean(),\n",
    "             'property_crime_1000': state_crime['property_crime_1000'].mean()}\n",
    "    sdevs = {'violent_crime_1000': state_crime['violent_crime_1000'].std(),\n",
    "             'property_crime_1000': state_crime['property_crime_1000'].std()}\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "\n",
    "    state_crime = pd.DataFrame(sc.fit_transform(state_crime), index=state_crime.index,columns=state_crime.columns)\n",
    "\n",
    "    forecasted = [years[-1],state, 'forecast']\n",
    "    \n",
    "    for crime in ['violent_crime_1000','property_crime_1000']:\n",
    "        state_crime_filtered = state_crime.copy()\n",
    "        state_crime_filtered = state_crime[['population', crime, 'avg_unemployment_rate', 'avg_CPI','ag_Democrat', 'ag_Mixed', 'ag_Republican']]\n",
    "        \n",
    "        #set the baseline value\n",
    "        crime_baseline_y = state_crime_filtered[crime][:-1]\n",
    "        \n",
    "        #define a wide window:\n",
    "        wide_window = WindowGenerator(\n",
    "            input_width=41, \n",
    "            label_width=41, \n",
    "            shift=1,\n",
    "            train_df=state_crime_filtered,\n",
    "            label_columns=[crime])\n",
    "\n",
    "        #define our LSTM model:\n",
    "        lstm_model = tf.keras.models.Sequential([\n",
    "            # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "            tf.keras.layers.LSTM(41, return_sequences=True),\n",
    "            # Shape => [batch, time, features]\n",
    "            tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "        \n",
    "        #fit:\n",
    "        history = compile_and_fit(lstm_model, wide_window)\n",
    "        \n",
    "        #model is now trained make a set of predictions:\n",
    "        predictive_input = WindowGenerator(\n",
    "            input_width=42, \n",
    "            label_width=42, \n",
    "            shift=0,\n",
    "            train_df=state_crime_filtered,\n",
    "            label_columns=[crime])\n",
    "        \n",
    "        #get predictions:\n",
    "        preds = lstm_model.predict(predictive_input.example[0])[0]\n",
    "        \n",
    "        #clear memory:\n",
    "        del history\n",
    "        \n",
    "        #get scores:\n",
    "        scores = scores.append(\n",
    "            get_scores(state=state,\n",
    "                       crime=crime,\n",
    "                       true=state_crime_filtered[crime][1:],\n",
    "                       baseline=crime_baseline_y,\n",
    "                       pred=preds[:-1]), ignore_index=True)\n",
    "        \n",
    "        #un-scale the preds so they are closer to their unscaled historical values:\n",
    "        preds = preds*sdevs[crime]+means[crime]\n",
    "        \n",
    "        #append rate predictions:\n",
    "        rates = pd.DataFrame(data={'year': years[1:],\n",
    "                                   'state': state,\n",
    "                                   'crime_rate': crime,\n",
    "                                   'prediction': [x[0] for x in preds]})\n",
    "        rate_preds = rate_preds.append(rates,ignore_index=True)\n",
    "        \n",
    "        #append forecasting forecasted crime data:\n",
    "        forecasted.append(preds[-1][0])\n",
    "    \n",
    "    #put the violent and property crime forecast into the forecasted dataframe\n",
    "    to_forecast = {x: None for x in target_forecast.columns}\n",
    "    for k, v in zip(to_forecast.keys(),forecasted):\n",
    "        to_forecast[k] = v\n",
    "    target_forecast = target_forecast.append(to_forecast,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MUxFC01Z7lzE"
   },
   "outputs": [],
   "source": [
    "#concatenate the historical and forecast dataframes together:\n",
    "historical = df.copy()\n",
    "historical = historical[['year','state_abbr','violent_crime_1000','property_crime_1000']]\n",
    "historical['measure_type'] = 'historical'\n",
    "historical = historical[['year', 'state_abbr','measure_type', 'violent_crime_1000', 'property_crime_1000']]\n",
    "target_forecast.rename(columns={'state': 'state_abbr'}, inplace=True) \n",
    "target_forecast = pd.concat([historical,target_forecast],axis=0).sort_values(['state_abbr','year']).reset_index()\n",
    "target_forecast.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gSciK2xDxamj"
   },
   "outputs": [],
   "source": [
    "#write stuff:\n",
    "target_forecast.to_csv('target_forecast.csv',index=False)\n",
    "scores.to_csv('scores.csv', index=False)\n",
    "rate_preds.to_csv('predictions.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
